* Kafka 的简介
** 什么是 Kafka
  Kafka 是一款分布式消息发布和订阅系统，具有高性能、高吞吐量的特点而被
  广泛应用与大数据传输场景。它是由 LinkedIn 公司开发，使用 Scala 语言编
  写，之后成为 Apache 基金会的一个顶级项目。kafka 提供了类似 JMS 的特
  性，但是在设计和实现上是完全不同的，而且他也不是 JMS 规范的实现。
** kafka 产生的背景
  kafka 作为一个消息系统，早起设计的目的是用作 LinkedIn 的活动流（Activity
  Stream）和运营数据处理管道（Pipeline）。活动流数据是所有的网站对用户
  的使用情况做分析的时候要用到的最常规的部分,活动数据包括页面的访问量
  （PV）、被查看内容方面的信息以及搜索内容。这种数据通常的处理方式是先
  把各种活动以日志的形式写入某种文件，然后周期性的对这些文件进行统计分
  析。运营数据指的是服务器的性能数据（CPU、IO 使用率、请求时间、服务日
  志等）。
** Kafka 的应用场景
  由于 kafka 具有更好的吞吐量、内置分区、冗余及容错性的优点(kafka 每秒可
  以处理几十万消息)，让 kafka 成为了一个很好的大规模消息处理应用的解决方
  案。所以在企业级应用长，主要会应用于如下几个方面
  + 行为跟踪：
    kafka 可以用于跟踪用户浏览页面、搜索及其他行为。通过发布订阅模式实时记录到对应的
    topic 中，通过后端大数据平台接入处理分析，并做更进一步的实时处理和监控
  + 日志收集：
    日志收集方面，有很多比较优秀的产品，比如 Apache Flume，很多公司使用
    kafka 代理日志聚合。日志聚合表示从服务器上收集日志文件，然后放到一个集中的平台（文
    件服务器）进行处理。在实际应用开发中，我们应用程序的 log 都会输出到本地的磁盘上，
    排查问题的话通过 linux 命令来搞定，如果应用程序组成了负载均衡集群，并且集群的机器
    有几十台以上，那么想通过日志快速定位到问题，就是很麻烦的事情了。所以一般都会做一
    个日志统一收集平台管理 log 日志用来快速查询重要应用的问题。所以很多公司的套路都是
    把应用日志几种到 kafka 上，然后分别导入到 es 和 hdfs 上，用来做实时检索分析和离线
    统计数据备份等。而另一方面，kafka 本身又提供了很好的 api 来集成日志并且做日志收集
** Kafka 本身的架构
  一个典型的 kafka 集群包含若干 Producer（可以是应用节点产生的消息，也可以是通过
  Flume 收集日志产生的事件），若干个 Broker（kafka 支持水平扩展）、若干个 Consumer
  Group，以及一个 zookeeper 集群。kafka 通过 zookeeper 管理集群配置及服务协同。
  Producer 使用 push 模式将消息发布到 broker，consumer 通过监听使用 pull 模式从
  broker 订阅并消费消息。
  多个 broker 协同工作，producer 和 consumer 部署在各个业务逻辑中。三者通过
  zookeeper 管理协调请求和转发。这样就组成了一个高性能的分布式消息发布和订阅系统。
  + *注意*: 与其他 mq 中间件不同的点，producer 发送消息到 broker
  的过程是 push，而 consumer 从 broker 消费消息的过程是 pull，主动去拉数
  据。而不是 broker 把数据主动发送给 consumer
  *注*: broker就是一个或者多个kafka服务器的统称
* kafka 的安装部署
  + 下载安装包
    https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.0/kafka_2.11-1.1.0.tgz
  + tar -zxvf 解压安装包
* kafka 目录介绍
  + /bin 操作 kafka 的可执行脚本
  + /config 配置文件
  + /libs 依赖库目录
  + /logs 日志数据目录
* Kafka 的基本操作
** 可以使用./xx命令 --help查看命令支持的参数
** 启动/停止 kafka
  + 需要先启动 zookeeper，如果没有搭建 zookeeper 环境，可以直接运行kafka 内嵌的 zookeeper
    启动命令： bin/zookeeper-server-start.sh config/zookeeper.properties &
  + 进入 kafka 目录，运行 bin/kafka-server-start.sh ｛-daemon 后台启动｝config/server.properties &
  + 进入 kafka 目录，运行 bin/kafka-server-stop.sh config/server.properties
** 创建 topic (类似一个博客订阅，刘亦菲的博客)
  ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
  + Replication-factor: 表示该 topic 需要在不同的 broker 中保存几份，这里设置成 1，表示在两个 broker 中保存两份
  + Partitions 分区数
** 查看 topic
  ./bin/kafka-topics.sh --list --zookeeper localhost:2181
** 查看 topic 属性
  ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
** 消费消息(屌丝a订阅主题, 而且还是很流氓的一直监听)
  ./bin/kafka-console-consumer.sh –bootstrap-server localhost:9092 --topic test --from-beginning
** 发送消息(女神菲发动态, 一发消息就被流氓a收到了)
  ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
* 安装集群环境
** 修改 server.properties 配置
  + 修改 server.properties. broker.id=0 / 1
  + 修改 server.properties 修改成本机 IP
    advertised.listeners=PLAINTEXT://192.168.11.153:9092
    当 Kafka broker 启动时，它会在 ZK 上注册自己的 IP 和端口号，客户端就通过
    这个 IP 和端口号来连接
** Kafka JAVA API 的使用
  https://github.com/offline7LY/demo/tree/master/kafka
* 配置信息分析
** 发送端的可选配置信息分析
*** acks
    acks 配置表示 producer 发送消息到 broker 上以后的确认值。有三个可选项
    + 0：表示 producer 不需要等待 broker 的消息确认。这个选项时延最小但同时风险最大（因为当 server 宕机时，数据将会丢失）。
    + 1：表示 producer 只需要获得 kafka 集群中的 leader 节点确认即可，这个选择时延较小同时确保了 leader 节点确认接收成功。
    + all(-1)：需要 ISR 中所有的 Replica 给予接收确认，速度最慢，安全性最高，但是由于 ISR 可能会缩小到仅包含一个 Replica，所以设置参数为 all 并不能一定避免数据丢失，
*** batch.size
    生产者发送多个消息到 broker 上的同一个分区时，为了减少网络请求带来的
    性能开销，通过批量的方式来提交消息，可以通过这个参数来控制批量提交的
    字节数大小，默认大小是 16384byte,也就是 16kb，意味着当一批消息大小达
    到指定的 batch.size 的时候会统一发送
*** linger.ms
    Producer 默认会把两次发送时间间隔内收集到的所有 Requests 进行一次聚合
    然后再发送，以此提高吞吐量，而 linger.ms 就是为每次发送到 broker 的请求
    增加一些 delay，以此来聚合更多的 Message 请求。 这个有点想 TCP 里面的
    Nagle 算法，在 TCP 协议的传输中，为了减少大量小数据包的发送，采用了
    Nagle 算法，也就是基于小包的等-停协议。
    *注意*: batch.size 和 linger.ms 这两个参数是 kafka 性能优化的关键参数，很多同
    学会发现 batch.size 和 linger.ms 这两者的作用是一样的，如果两个都配置
    了，那么怎么工作的呢？实际上，当二者都配置的时候，只要满足其中一个要
    求，就会发送请求到 broker 上
*** max.request.size
    设置请求的数据的最大字节数，为了防止发生较大的数据包影响到吞吐量，默认值为 1MB。
** 消费端的可选配置分析
*** group.id
    consumer group 是 kafka 提供的可扩展且具有容错性的消费者机制。既然是
    一个组，那么组内必然可以有多个消费者或消费者实例(consumer instance)，
    它们共享一个公共的 ID，即 group ID。组内的所有消费者协调在一起来消费订
    阅主题(subscribed topics)的所有分区(partition)。当然，每个分区只能由同一
    个消费组内的一个 consumer 来消费.如下图所示，分别有三个消费者，属于两
    个不同的 group，那么对于 firstTopic 这个 topic 来说，这两个组的消费者都
    能同时消费这个 topic 中的消息，对于此事的架构来说，这个 firstTopic 就类
    似于 ActiveMQ 中的 topic 概念。如果 3 个消费者都属于同一个
    group，那么此事 firstTopic 就是一个 Queue 的概念
*** enable.auto.commit
    消费者消费消息以后自动提交，只有当消息提交以后，该消息才不会被再次接
    收到，还可以配合 auto.commit.interval.ms 控制自动提交的频率。
    当然，我们也可以通过 consumer.commitSync()的方式实现手动提交
*** auto.offset.reset
    这个参数是针对新的 groupid 中的消费者而言的，当有新 groupid 的消费者来
    消费指定的 topic 时，对于该参数的配置，会有不同的语义
    auto.offset.reset=latest 情况下，新的消费者将会从其他消费者最后消费的
    offset 处开始消费 Topic 下的消息
    auto.offset.reset= earliest 情况下，新的消费者会从该 topic 最早的消息开始
    消费
    auto.offset.reset=none 情况下，新的消费者加入以后，由于之前不存在
    offset，则会直接抛出异常。
*** max.poll.records
    此设置限制每次调用 poll 返回的消息数，这样可以更容易的预测每次 poll 间隔
    要处理的最大值。通过调整此值，可以减少 poll 间隔
* spring-kafka 集成
1. Topic&Partition
2. 消息分发策略
3. 消息消费原理
4. 消息的存储策略
5. Partition 副本机制
* 关于 Topic 和 Partition
** Topic
  在 kafka 中，topic 是一个存储消息的逻辑概念，可以认为
  是一个消息集合。每条消息发送到 kafka 集群的消息都有
  一个类别。物理上来说，不同的 topic 的消息是分开存储
  的，每个 topic 可以有多个生产者向它发送消息，也可以有多
  个消费者去消费其中的消息。
** Partition
  每个 topic 可以划分多个分区（每个 Topic 至少有一个分
  区），同一 topic 下的不同分区包含的消息是不同的。每个
  消息在被添加到分区时，都会被分配一个 offset（称之为偏
  移量），它是消息在此分区中的唯一编号，kafka 通过 offset
  保证消息在分区内的顺序，offset 的顺序不跨分区，即 kafka
  只保证在同一个分区内的消息是有序的。

  每一条消息发送到 broker 时，会根据 partition 的规则
  选择存储到哪一个 partition。如果 partition 规则设置合
  理，那么所有的消息会均匀的分布在不同的partition中，
  这样就有点类似数据库的分库分表的概念，把数据做了分片处理。
** Topic&Partition 的存储
  Partition 是以文件的形式存储在文件系统中，比如创建一
  个名为 firstTopic 的 topic，其中有 3 个 partition，那么在
  kafka 的数据目录（/tmp/kafka-log）中就有 3 个目录，
  firstTopic-0~3，命名规则是<topic_name>-<partition_id>
  ./kafka-topics.sh --create --zookeeper 192.168.11.156:2181 --replication-factor 1 --partitions 3 --topic firstTopic
* 关于消息分发
** kafka 消息分发策略
  消息是 kafka 中最基本的数据单元，在 kafka 中，一条消息
  由 key、value 两部分构成，在发送一条消息时，我们可以
  指定这个 key，那么 producer 会根据 key 和 partition 机
  制来判断当前这条消息应该发送并存储到哪个 partition 中。
  我们可以根据需要进行扩展 producer 的 partition 机制。
** 消息默认的分发机制
  默认情况下，kafka 采用的是 hash 取模的分区算法。如果
  Key 为 null，则会随机分配一个分区。这个随机是在这个参
  数”metadata.max.age.ms”的时间范围内随机选择一个。对
  于这个时间段内，如果 key 为 null，则只会发送到唯一的
  分区。这个值值哦默认情况下是 10 分钟更新一次。

  关 于 Metadata ，这个之前没讲过，简单理解就是
  Topic/Partition 和 broker 的映射关系，每一个 topic 的每
  一个 partition，需要知道对应的 broker 列表是什么，leader
  是谁、follower 是谁。这些信息都是存储在 Metadata 这个类里面。
** 消费端如何消费指定的分区
  通过下面的代码，就可以消费指定该 topic 下的 0 号分区。
  其他分区的数据就无法接收
  #+BEGIN_EXAMPLE
  //消费指定分区的时候，不需要再订阅
  //kafkaConsumer.subscribe(Collections.singletonList(topic));
  //消费指定的分区
  TopicPartition topicPartition=new TopicPartition(topic,0);
  kafkaConsumer.assign(Arrays.asList(topicPartition));
  #+END_EXAMPLE
** 消息的消费原理
  + kafka 消息消费原理演示
    在实际生产过程中，每个 topic 都会有多个 partitions，多
    个 partitions 的好处在于，一方面能够对 broker 上的数据
    进行分片有效减少了消息的容量从而提升 io 性能。另外一
    方面，为了提高消费端的消费能力，一般会通过多个
    consumer 去消费同一个 topic ，也就是消费端的负载均
    衡机制，也就是我们接下来要了解的，在多个 partition 以
    及多个 consumer 的情况下，消费者是如何消费消息的
    同时，kafka 存在 consumer group
    的 概 念 ， 也 就是 group.id 一 样 的 consumer ，这些
    consumer 属于一个 consumer group，组内的所有消费者
    协调在一起来消费订阅主题的所有分区。当然每一个分区
    只能由同一个消费组内的 consumer 来消费，那么同一个
    consumer group 里面的 consumer 是怎么去分配该消费
    哪个分区里的数据的呢？如下图所示，3 个分区，3 个消费
    者，那么哪个消费者消分哪个分区？
    对于上面这个图来说，这 3 个消费者会分别消费 test 这个
    topic 的 3 个分区，也就是每个 consumer 消费一个
    partition。
** 什么是分区分配策略
  通过前面的案例演示，我们应该能猜到，同一个 group 中
  的消费者对于一个 topic 中的多个 partition，存在一定的
  分区分配策略。
  在 kafka 中，存在两种分区分配策略，一种是 Range(默认)、
  另 一 种 另 一 种 还 是 RoundRobin （ 轮 询 ）。 通 过
  partition.assignment.strategy 这个参数来设置。
  + Range strategy（范围分区）
    Range 策略是对每个主题而言的，首先对同一个主题里面
    的分区按照序号进行排序，并对消费者按照字母顺序进行
    排序。假设我们有 10 个分区，3 个消费者，排完序的分区
    将会是 0, 1, 2, 3, 4, 5, 6, 7, 8, 9；消费者线程排完序将会是
    C1-0, C2-0, C3-0。然后将 partitions 的个数除于消费者线
    程的总数来决定每个消费者线程消费几个分区。如果除不
    尽，那么前面几个消费者线程将会多消费一个分区。在我
    们的例子里面，我们有 10 个分区，3 个消费者线程， 10 /
    3 = 3，而且除不尽，那么消费者线程 C1-0 将会多消费一
    个分区，所以最后分区分配
    的结果看起来是这样的：
    C1-0 将消费 0, 1, 2, 3 分区
    C2-0 将消费 4, 5, 6 分区
    C3-0 将消费 7, 8, 9 分区
    假如我们有 11 个分区，那么最后分区分配的结果看起来是这样的：
    C1-0 将消费 0, 1, 2, 3 分区
    C2-0 将消费 4, 5, 6, 7 分区
    C3-0 将消费 8, 9, 10 分区
    假如我们有 2 个主题(T1 和 T2)，分别有 10 个分区，那么最后
    分区分配的结果看起来是这样的：
    C1-0 将消费 T1 主题的 0, 1, 2, 3 分区以及 T2 主题的 0,
    1, 2, 3 分区
    C2-0 将消费 T1 主题的 4, 5, 6 分区以及 T2 主题的 4, 5,
    6 分区
    C3-0 将消费 T1 主题的 7, 8, 9 分区以及 T2 主题的 7, 8,
    9 分区
    可以看出，C1-0 消费者线程比其他消费者线程多消费了 2 个
    分区，这就是 Range strategy 的一个很明显的弊端
  + RoundRobin strategy（轮询分区）
      轮询分区策略是把所有 partition 和所有 consumer 线程都
      列出来，然后按照 hashcode 进行排序。最后通过轮询算
      法分配 partition 给消费线程。如果所有 consumer 实例的
      订阅是相同的，那么 partition 会均匀分布。
      在我们的例子里面，假如按照 hashCode 排序完的 topicpartitions
      组依次为 T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, 
      T1-7, T1-6, T1-9，我们的消费者线程排序为 C1-0, C1-1, C2-
      0, C2-1，最后分区分配的结果为：
      C1-0 将消费 T1-5, T1-2, T1-6 分区；
      C1-1 将消费 T1-3, T1-1, T1-9 分区；
      C2-0 将消费 T1-0, T1-4 分区；
      C2-1 将消费 T1-8, T1-7 分区；
      使用轮询分区策略必须满足两个条件
      1. 每个主题的消费者实例具有相同数量的流
      2. 每个消费者订阅的主题必须是相同的
      什么时候会触发这个策略呢？
      当出现以下几种情况时，kafka 会进行一次分区分配操作，
      也就是 kafka consumer 的 rebalance
      1. 同一个 consumer group 内新增了消费者
      2. 消费者离开当前所属的 consumer group，比如主动停
      机或者宕机
      3. topic 新增了分区（也就是分区数量发生了变化）
      kafka consuemr 的 rebalance 机制规定了一个 consumer
      group 下的所有 consumer 如何达成一致来分配订阅 topic
      的每个分区。而具体如何执行分区策略，就是前面提到过
      的两种内置的分区策略。而 kafka 对于分配策略这块，提
      供了可插拔的实现方式， 也就是说，除了这两种之外，我们还可以创建自己的分配机制。
  + 谁来执行 Rebalance 以及管理 consumer 的 group 呢？
      Kafka 提供了一个角色：coordinator 来执行对于 consumer
      group 的管理，Kafka 提供了一个角色：coordinator 来执
      行对于 consumer group 的管理，当 consumer group 的
      第一个 consumer 启动的时候，它会去和 kafka server 确
      定谁是它们组的 coordinator。之后该 group 内的所有成
      员都会和该 coordinator 进行协调通信
  + 如何确定 coordinator
    consumer group 如何确定自己的 coordinator 是谁呢, 消
    费 者 向 kafka 集 群 中 的 任 意 一 个 broker 发 送 一 个
    GroupCoordinatorRequest 请求，服务端会返回一个负载
    最 小 的 broker 节 点 的 id ， 并 将 该 broker 设 置 为coordinator
  + JoinGroup 的过程
    在 rebalance 之前，需要保证 coordinator 是已经确定好了
    的，整个 rebalance 的过程分为两个步骤，Join 和 Sync
    join: 表示加入到 consumer group 中，在这一步中，所有
    的成员都会向 coordinator 发送 joinGroup 的请求。一旦
    所有成员都发送了 joinGroup 请求，那么 coordinator 会
    选择一个 consumer 担任 leader 角色，并把组成员信息和
    订阅信息发送消费者

    protocol_metadata: 序列化后的消费者的订阅信息
    leader_id： 消费组中的消费者，coordinator 会选择一个
    座位 leader，对应的就是 member_id
    member_metadata 对应消费者的订阅信息
    members：consumer group 中全部的消费者的订阅信息
    generation_id：年代信息，类似于之前讲解 zookeeper 的
    时候的 epoch 是一样的，对于每一轮 rebalance ，
    generation_id 都会递增。主要用来保护 consumer group。
    隔离无效的 offset 提交。也就是上一轮的 consumer 成员
    无法提交 offset 到新的 consumer group 中。
  + Synchronizing Group State 阶段
    完成分区分配之后，就进入了 Synchronizing Group State
    阶段，主要逻辑是向 GroupCoordinator 发 送
    SyncGroupRequest 请求，并且处理 SyncGroupResponse
    响应，简单来说，就是 leader 将消费者对应的 partition 分
    配方案同步给 consumer group 中的所有 consumer
    每个消费者都会向 coordinator 发送 syncgroup 请求，不
    过只有 leader 节点会发送分配方案，其他消费者只是打打
    酱油而已。当 leader 把方案发给 coordinator 以后，
    coordinator 会把结果设置到 SyncGroupResponse 中。这
    样所有成员都知道自己应该消费哪个分区。
    *注意* consumer group 的分区分配方案是在客户端执行的！
    Kafka 将这个权利下放给客户端主要是因为这样做可以
    有更好的灵活性
* 如何保存消费端的消费位置
** 什么是 offset
  前面在讲解 partition 的时候，提到过 offset， 每个 topic
  可以划分多个分区（每个 Topic 至少有一个分区），同一
  topic 下的不同分区包含的消息是不同的。每个消息在被添
  加到分区时，都会被分配一个 offset（称之为偏移量, 类似与平时业务数据的guid），它
  是消息在此分区中的唯一编号，kafka 通过 offset 保证消息
  在分区内的顺序，offset 的顺序不跨分区，即 kafka 只保证
  在同一个分区内的消息是有序的； 对于应用层的消费来说，
  每次消费一个消息并且提交以后，会保存当前消费到的最
  近的一个 offset。那么 offset 保存在哪里？
** offset 在哪里维护？
  在 kafka 中，提供了一个__consumer_offsets_* 的一个
  topic ， 把 offset 信 息 写 入 到 这 个 topic 中 。
  __consumer_offsets——按保存了每个 consumer group
  某一时刻提交的 offset 信息。__consumer_offsets 默认有
  50 个分区。
  根据前面我们演示的案例，我们设置了一个
  KafkaConsumerDemo 的 groupid。首先我们需要找到这
  个 consumer_group 保存在哪个分区中
  properties.put(ConsumerConfig.GROUP_ID_CONFIG,"KafkaConsumerDemo");
  计算公式
  + Math.abs(“groupid”.hashCode())%groupMetadataTopicPartitionCount 
  ; 由于默认情况下groupMetadataTopicPartitionCount 有 50 个分区，计
  算得到的结果为:35, 意味着当前的 consumer_group 的
  位移信息保存在__consumer_offsets 的第 35 个分区
  + 执行如下命令，可以查看当前 consumer_goup 中的offset 位移信息
    #+BEGIN_EXAMPLE
    sh kafka-simple-consumer-shell.sh --topic --consumer_offsets --partition 35 --broker-list 192.168.11.153:9092,192.168.11.154:9092,192.168.11.157:9092 --formatter "kafka.coordinator.group.GroupMetadataManagerOffsetsMessageFormatter"
    #+END_EXAMPLE
    从输出结果中，我们就可以看到 test 这个 topic 的 offset的位移日志
* 消息的存储
** 消息的保存路径
  消息发送端发送消息到 broker 上以后，消息是如何持久化
  的呢？那么接下来去分析下消息的存储
  首先我们需要了解的是，kafka 是使用日志文件的方式来保
  存生产者和发送者的消息，每条消息都有一个 offset 值来
  表示它在分区中的偏移量。Kafka 中存储的一般都是海量的
  消息数据，为了避免日志文件过大，Log 并不是直接对应
  在一个磁盘上的日志文件，而是对应磁盘上的一个目录，
  这个目录的明明规则是<topic_name>_<partition_id>
  比如创建一个名为 firstTopic 的 topic，其中有 3 个 partition，
  那么在 kafka 的数据目录（/tmp/kafka-log）中就有 3 个目
  录，firstTopic-0~3
** 多个分区在集群中的分配
  如果我们对于一个 topic，在集群中创建多个 partition，那
  么 partition 是如何分布的呢？
  1.将所有 N Broker 和待分配的 i 个 Partition 排序
  2.将第 i 个 Partition 分配到第(i mod n)个 Broker 上
  了解到这里的时候，大家再结合前面讲的消息分发策略，
  就应该能明白消息发送到 broker 上，消息会保存到哪个分
  区中，并且消费端应该消费哪些分区的数据了。
** 消息写入的性能
  我们现在大部分企业仍然用的是机械结构的磁盘，如果把
  消息以随机的方式写入到磁盘，那么磁盘首先要做的就是
  寻址，也就是定位到数据所在的物理地址，在磁盘上就要
  找到对应的柱面、磁头以及对应的扇区；这个过程相对内
  存来说会消耗大量时间，为了规避随机读写带来的时间消
  耗，kafka 采用顺序写的方式存储数据。即使是这样，但是
  频繁的 I/O 操作仍然会造成磁盘的性能瓶颈，所以 kafka
  还有一个性能策略
  + 零拷贝
    消息从发送到落地保存，broker 维护的消息日志本身就是
    文件目录，每个文件都是二进制保存，生产者和消费者使
    用相同的格式来处理。在消费者获取消息时，服务器先从
    硬盘读取数据到内存，然后把内存中的数据原封不动的通
    过 socket 发送给消费者。虽然这个操作描述起来很简单，
    但实际上经历了很多步骤。
    + 操作系统将数据从磁盘读入到内核空间的页缓存
    + 应用程序将数据从内核空间读入到用户空间缓存中
    + 应用程序将数据写回到内核空间到 socket 缓存中
    + 操作系统将数据从 socket 缓冲区复制到网卡缓冲区，以便将数据经网络发出这个过程涉及到 4 次上下文切换以及 4 次数据复制，并且有两次复制操作是由 CPU 完成。但是这个过程中，数据完全没有进行变化，仅仅是从磁盘复制到网卡缓冲区。
    通过“零拷贝”技术，可以去掉这些没必要的数据复制操作，
    同时也会减少上下文切换次数。现代的 unix 操作系统提供
    一个优化的代码路径，用于将数据从页缓存传输到 socket；
    在 Linux 中，是通过 sendfile 系统调用来完成的。Java 提
    供了访问这个系统调用的方法：FileChannel.transferTo API
    使用 sendfile，只需要一次拷贝就行，允许操作系统将数据
    直接从页缓存发送到网络上。所以在这个优化的路径中，
    只有最后一步将数据拷贝到网卡缓存中是需要的
