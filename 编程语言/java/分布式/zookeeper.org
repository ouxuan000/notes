* 分布式协调服务-zookeeper
* 分布式环境的特点
** 分布性
** 并发性
  + 程序运行过程中，并发性操作是很常见的。比如同一个分布式系统中的多个节点，同时访问一个共享资源。数据库、分布式存储
** 无序性
  + 进程之间的消息通信，会出现顺序不一致问题
* 分布式环境下面临的问题
** 网络通信
  + 网络本身的不可靠性，因此会涉及到一些网络通信问题
** 网络分区(脑裂)
+ 当网络发生异常导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式架构的所有节点，只有部分节点能够正常通信
** 三态
+ 在分布式架构里面，除了成功、失败、超时
* 分布式事务
+ ACID(原子性、一致性、隔离性、持久性)
* 中心化和去中心化
  + 冷备或者热备
    分布式架构里面，很多的架构思想采用的是：当集群发生故障的时候，集群中的人群会自动“选举”出一个新的领导。最典型的是： zookeeper / etcd

* 经典的CAP/BASE理论
** CAP
  + C（一致性 Consistency）: 所有节点上的数据，时刻保持一致
  + 可用性（Availability）：每个请求都能够收到一个响应，无论响应成功或者失败
  + 分区容错 （Partition-tolerance）：表示系统出现脑裂以后，可能导致某些server与集群中的其他机器失去联系
  + CP  / AP
  + CAP理论仅适用于原子读写的Nosql场景，不适用于数据库系统
** BASE
+ 基于CAP理论，CAP理论并不适用于数据库事务（因为更新一些错误的数据而导致数据出现紊乱，无论什么样的数据库高可用方案都是
徒劳） ，虽然XA事务可以保证数据库在分布式系统下的ACID特性，但是会带来性能方面的影响；
+ eBay尝试了一种完全不同的套路，放宽了对事务ACID的要求。提出了BASE理论
Basically available  ： 数据库采用分片模式， 把100W的用户数据分布在5个实例上。如果破坏了其中一个实例，仍然可以保证
80%的用户可用
+ soft-state：  在基于client-server模式的系统中，server端是否有状态，决定了系统是否具备良好的水平扩展、负载均衡、故障恢复等特性。
Server端承诺会维护client端状态数据，这个状态仅仅维持一小段时间, 这段时间以后，server端就会丢弃这个状态，恢复正常状态
+ Eventually consistent：数据的最终一致性
* 初步认识zookeeper
+ zookeeper是一个开源的分布式协调服务，是由雅虎创建的，基于google chubby。
+ 一致，有头， 数据树
** zookeeper是什么
+ 分布式数据一致性的解决方案
** zookeeper能做什么
+ 数据的发布/订阅（配置中心:disconf）  、 负载均衡（dubbo利用了zookeeper机制实现负载均衡） 、命名服务、master选举(kafka、hadoop、hbase)、分布式队列、分布式锁
** zookeeper的特性
*** 顺序一致性
+ 从同一个客户端发起的事务请求，最终会严格按照顺序被应用到zookeeper中
*** 原子性
+ 所有的事务请求的处理结果在整个集群中的所有机器上的应用情况是一致的，也就是说，要么整个集群中的所有机器都成功应用了某一事务、要么全都不应用
*** 可靠性
+ 一旦服务器成功应用了某一个事务数据，并且对客户端做了响应，那么这个数据在整个集群中一定是同步并且保留下来的
*** 实时性
+ 一旦一个事务被成功应用，客户端就能够立即从服务器端读取到事务变更后的最新数据状态；（zookeeper仅仅保证在一定时间内，近实时）
* zookeeper安装
** 单机环境安装
+ 下载zookeeper的安装包
http://apache.fayea.com/zookeeper/stable/zookeeper-3.4.10.tar.gz
+ 解压zookeeper 
tar -zxvf zookeeper-3.4.10.tar.gz
+ cd 到 ZK_HOME/conf  , copy一份zoo.cfg
cp  zoo_sample.cfg  zoo.cfg
+ sh zkServer.sh
{start|start-foreground|stop|restart|status|upgrade|print-cmd}
+ sh zkCli.sh -server  ip:port
** 集群环境
zookeeper集群, 包含三种角色: leader / follower /observer
*** observer
  observer 是一种特殊的zookeeper节点。可以帮助解决zookeeper的扩展性（如果大量客户端访问我们zookeeper集群，需要增加zookeeper集群机器数量。从而增加zookeeper集群的性能。 导致zookeeper写性能下降， zookeeper的数据变更需要半数以上服务器投票通过。造成网络消耗增加投票成本）
1. observer不参与投票。 只接收投票结果。
2. 不属于zookeeper的关键部位。
3. 在zoo.cfg里面增加
peerType=observer
  #+BEGIN_EXAMPLE
  server.1=192.168.140.128:2888:3888:observer
  server.2=192.168.140.129:2888:3888
  server.3=192.168.140.130:2888:3888
  #+END_EXAMPLE
*** zookeeper服务配置启动
+ 修改配置文件
  + copy $zookeeper_home/conf/zoo_simple.cfg zoo.cfg
  + 编辑zoo.cfg文件, 在底部加入如下配置(相当于对所有可用的服务做一个ip映射)
  #+BEGIN_EXAMPLE
  #id的取值范围： 1~255； 用id来标识该机器在集群中的机器序号
  #server.id=host:port:port
  #clientPort:2181是zookeeper的默认端口； //3306
  server.1=192.168.140.128:2888:3888
  server.2=192.168.140.129:2888:3888
  server.3=192.168.140.130:2888:3888
  #3888表示leader选举的端口
  #2888表示服务器通信端口
  #+END_EXAMPLE
+ 创建myid
  #+BEGIN_EXAMPLE
  在每一个服务器的dataDir目录下创建一个myid的文件，文件就一行数据，数据内容是每台机器对应的server ID的数字
  例如: touch /tmp/zookeeper/myid, echo 1>/tmp/zookeeper/myid #这个文件是为了标识当前服务是上面注册的哪个服务， id对应
  #+END_EXAMPLE
+ 启动zookeeper
  + 打开xshell窗口，左下角设置命令所有session生效，并在底行执行 sh $zookeeper_home/bin/zkServer.sh start
  + 使用sh $zookeeper_home/bin/zkServer.sh status检测服务是否启动成功, 如果出现: Mode:follower\leader\..., 说明启动成功
*** 客户端连接
  + 任意窗口 $zk_home/bin/sh./zkCli.sh
  + 查看目录树 ls /
  + create [-s] [-e] path data acl
    -s 表示节点是否有序
    -e 表示是否为临时节点
    默认情况下，是持久化节点
    例子: 创建一个目录 create /tt 123
  + get path [watch]
    获得指定 path的信息
    例:获取值 get /tt
  + set path data [version]
    修改节点 path对应的data
    zookeeper 会带有一个锁, 改过以后上个版本就不能修改了
    例子: create /tt 123 (version 0); set /tt 456(version 1); set /tt 789 0(操作0版本的/tt)会报错
  + delete path [version]
    删除节点
  + stat信息
    cversion = 0       子节点的版本号
    aclVersion = 0     表示acl的版本号，修改节点权限
    dataVersion = 1    表示的是当前节点数据的版本号

    czxid    节点被创建时的事务ID
    mzxid   节点最后一次被更新的事务ID
    pzxid    当前节点下的子节点最后一次被修改时的事务ID

    ctime = Sat Aug 05 20:48:26 CST 2017
    mtime = Sat Aug 05 20:48:50 CST 2017
    cZxid = 0x500000015
    ctime = Sat Aug 05 20:48:26 CST 2017
    mZxid = 0x500000016
    mtime = Sat Aug 05 20:48:50 CST 2017
    pZxid = 0x500000015
    cversion = 0
    dataVersion = 1
    aclVersion = 0
    ephemeralOwner = 0x0   创建临时节点的时候，会有一个sessionId 。 该值存储的就是这个sessionid
    dataLength = 3    数据值长度
    numChildren = 0  子节点数
*** 出现的问题
  + 启动后查看status发现启动不成功, 查看bin目录下的zookeeper.out文件进行分析
    + connect timeout: 关闭防火墙: systemctl stop firewalld.service； 清理原 /tmp/zookeeper/下除myid的文件； 有些网友发现可能还与hosts文件中127.0.0.1有关，注释掉即可(需要测试, 我这里是注释掉了)
    + 端口占用: ps -aux |grep zookeeper  杀掉所有进程重新启动
* java API的使用
  + 导入jar包
    #+BEGIN_EXAMPLE
      <dependency>
          <groupId>org.apache.zookeeper</groupId>
          <artifactId>zookeeper</artifactId>
          <version>3.4.8</version>
      </dependency>
    #+END_EXAMPLE
  + 创建一个api的zookeeper连接
    #+BEGIN_SRC java
      /**
          * 获取连接
          * @return
          * @throws IOException
          * @throws InterruptedException
          */
          public static ZooKeeper getInstance() throws IOException, InterruptedException {
              final CountDownLatch countDownLatch = new CountDownLatch(1);
              ZooKeeper zooKeeper=new ZooKeeper(CONNECTSTRING, sessionTimeout, new Watcher() {
                  public void process(WatchedEvent event) {
                      if(event.getState()== Event.KeeperState.SyncConnected){
                          switch (event.getType()) {
                              case None:
                                  //第一次进入
                                  System.out.println("连接成功---------" + event);
                                  //使用countDown保证连接成功，连接也需要时间, 只有判断连接上了， 才会释放锁
                                  conectStatus.countDown();
                                  break;
                          }
                      }
                  }
              });
              countDownLatch.await();
              return zooKeeper;
          }
    #+END_SRC
  + api实现增删改查
    #+BEGIN_SRC java
        zooKeeper = getInstance();
        // 增删改查, 临时节点
        zooKeeper.create("/xxtt", "xxx".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);
        //注册对这个节点的监听事件
        zooKeeper.getData("/xxtt", new ZookeeperWatcher(), new Stat());

        zooKeeper.setData("/xxtt", "i love you".getBytes(), -1);
        //delete 和create 临时节点没有触发watch事件
        zooKeeper.delete("/xxtt", -1);
    #+END_SRC
  + 完整代码见: https://github.com/offline7LY/demo/blob/master/zookeeper/src/main/java/com/lx/demo/javaapi/ZookeeperClient.java 
  + 权限控制模式
    #+BEGIN_EXAMPLE
      schema：授权对象
      ip     : 192.168.1.1
      Digest  : username:password
      world  : 开放式的权限控制模式，数据节点的访问权限对所有用户开放。 world:anyone
      super  ：超级用户，可以对zookeeper上的数据节点进行操作
    #+END_EXAMPLE
  + 连接状态
    #+BEGIN_EXAMPLE
      KeeperStat.Expired  在一定时间内客户端没有收到服务器的通知， 则认为当前的会话已经过期了。
      KeeperStat.Disconnected  断开连接的状态
      KeeperStat.SyncConnected  客户端和服务器端在某一个节点上建立连接，并且完成一次version、zxid同步
      KeeperStat.authFailed  授权失败
    #+END_EXAMPLE
  + 事件类型
    #+BEGIN_EXAMPLE
      NodeCreated  当节点被创建的时候，触发
      NodeChildrenChanged  表示子节点被创建、被删除、子节点数据发生变化
      NodeDataChanged    节点数据发生变化
      NodeDeleted        节点被删除
      None   客户端和服务器端连接状态发生变化的时候，事件类型就是None
    #+END_EXAMPLE
* zkclient
  + 导入jar包
    #+BEGIN_EXAMPLE
      <!-- https://mvnrepository.com/artifact/com.101tec/zkclient -->
      <dependency>
          <groupId>com.101tec</groupId>
          <artifactId>zkclient</artifactId>
          <version>0.10</version>
      </dependency>
    #+END_EXAMPLE
  + 建立连接
    #+BEGIN_SRC java
        /**
        * 服务器配置的zkserver， 其中ip地址为各个服务器的ip地址, 2181端口为客户端访问服务器的端口
        */
        private final static String CONNECTSTRING="192.168.140.128:2181,192.168.140.129:2181," +
                "192.168.140.130:2181";
        //内部已经进行了加锁处理， 保证正常获取连接
        ZkClient zkClient = new ZkClient(CONNECTSTRING, sessionTimeout);
    #+END_SRC
  + 增删改查
    #+BEGIN_SRC java
        //递归创建持久化节点
        zkClient.createPersistent("/xxtt/xx-01/xx-001", true);
        //改
        zkClient.writeData("/xxtt", "i love you");
        //查
        List<String> children = zkClient.getChildren("/xxtt");
        System.out.println(children);
        //递归删除
        zkClient.deleteRecursive("/xxtt");
    #+END_SRC
  + 添加事件监听
    #+BEGIN_SRC java
      //注册指定节点的修改和删除事件, 注： 这里可能会因为主线程过早结束导致异步操作无法输出，
      //所以可以在主线程最后加入 TimeUnit.SECONDS.sleep(3);来延时
      zkClient.subscribeDataChanges("/xxtt", new IZkDataListener() {
          public void handleDataChange(String s, Object o) throws Exception {
              System.out.println(s + " is changed!!");
          }

          public void handleDataDeleted(String s) throws Exception {
              System.out.println(s + " is deleted!!");
          }
      });
    #+END_SRC
  + 代码路径: https://github.com/offline7LY/demo/tree/master/zookeeper/src/main/java/com/lx/demo/zkclient
* curator
  + 简介
    #+BEGIN_EXAMPLE
      Curator本身是Netflix公司开源的zookeeper客户端；
      curator提供了各种应用场景的实现封装
      curator-framework  提供了fluent风格api
      curator-replice     提供了实现封装

      curator连接的重试策略
      ExponentialBackoffRetry()  衰减重试 
      RetryNTimes 指定最大重试次数
      RetryOneTime 仅重试一次
      RetryUnitilElapsed 一直重试知道规定的时间
    #+END_EXAMPLE
  + 导入jar包
    #+BEGIN_EXAMPLE
        <!-- curator 测试使用 begin-->
        <!-- https://mvnrepository.com/artifact/org.apache.curator/curator-framework -->
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>4.0.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.curator/curator-recipes -->
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-recipes</artifactId>
            <version>4.0.0</version>
        </dependency>
        <!-- curator 测试使用 end-->
    #+END_EXAMPLE
  + 建立连接
    + normal
      #+BEGIN_SRC java
        CuratorFramework curatorFramework = CuratorFrameworkFactory.newClient(CONNECTSTRING, sessionTimeout,
                5000, new ExponentialBackoffRetry(1000, 3));
        //curator的方式都需要start以后才会启动
        curatorFramework.start();
      #+END_SRC
    + fluent
      #+BEGIN_SRC java
        //fluent 链式风格
        CuratorFramework zkWithCuratorBuild = CuratorFrameworkFactory
                .builder()
                .connectString(CONNECTSTRING)
                .retryPolicy(new ExponentialBackoffRetry(1000, 3))
                .build();
        zkWithCuratorBuild.start();
      #+END_SRC
  + 增删改查
    + 同步操作
      #+BEGIN_SRC java
          // 增删改查 同步操作
          //递归创建(creatingParentContainersIfNeeded)持久化节点
          String s = zkWithCuratorBuild.create()
                  .creatingParentContainersIfNeeded()
                  .withMode(CreateMode.PERSISTENT)
                  .forPath("/xxtt/tt/tt-01", "123".getBytes());
          System.out.println(s);
          //改
          Stat stat = zkWithCuratorBuild.setData().forPath("/xxtt", "456".getBytes());
          System.out.println(stat);
          //查
          Stat stat1 = new Stat();
          byte[] bytes = zkWithCuratorBuild.getData().storingStatIn(stat1).forPath("/xxtt");
          System.out.println(bytes + " stat: " + stat1);
          //删除 (.deletingChildrenIfNeeded 递归删除)
          zkWithCuratorBuild.delete().deletingChildrenIfNeeded().forPath("/xxtt");
      #+END_SRC
    + 异步操作
      #+BEGIN_SRC java
        zkWithCuratorBuild.create().creatingParentContainersIfNeeded()
                .withMode(CreateMode.PERSISTENT)
                .inBackground(new BackgroundCallback() {
                    public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception {
                        System.out.println("节点被创建: name: " + curatorEvent.getPath() + " type:" + curatorEvent.getType());

                        zkWithCuratorBuild.delete().deletingChildrenIfNeeded().forPath("/xxtt");
                        System.out.println("节点删掉了");
                    }
                }).forPath("/xxtt", "987".getBytes());
        //给异步操作一个缓冲事件 实现方式很多{system.in.read();  countlaunch ....}
        TimeUnit.SECONDS.sleep(3)
      #+END_SRC
  + 事务处理
    #+BEGIN_SRC java
        Collection<CuratorTransactionResult> curatorTransactionResults = zkWithCuratorBuild.inTransaction().create().forPath("/xxtt", "111".getBytes())
                .and()
                .setData().forPath("/tt", "2222".getBytes())
                .and()
                .commit();
        for (CuratorTransactionResult curatorTransactionResult : curatorTransactionResults) {
            System.out.println(curatorTransactionResult.getForPath() + " ---->> " + curatorTransactionResult.getType());
        }
        zkWithCuratorBuild.delete().deletingChildrenIfNeeded().forPath("/xxtt");
    #+END_SRC
  + watch机制
    + nodecache 监视一个节点的创建、更新、删除
      #+BEGIN_SRC java
        NodeCache nodeCache = new NodeCache(curatorFramework, "/xxtt");
        nodeCache.start();
        nodeCache.getListenable().addListener(()-> System.out.println("NodeCache: 节点发生了变化-------"));

        curatorFramework.create().forPath("/xxtt", "111".getBytes());
        curatorFramework.setData().forPath("/xxtt", "6666".getBytes());
        curatorFramework.delete().forPath("/xxtt");
        TimeUnit.SECONDS.sleep(3);
      
      #+END_SRC
    + PathChildrenCache 监视一个路径下子节点的创建、删除、节点数据更新
      #+BEGIN_SRC java
        PathChildrenCache pathChildrenCache = new PathChildrenCache(curatorFramework, "/xxtt", true);
        pathChildrenCache.start(PathChildrenCache.StartMode.POST_INITIALIZED_EVENT);
        pathChildrenCache.getListenable().addListener((curatorFramework1, pathChildrenCacheEvent) -> {
            switch (pathChildrenCacheEvent.getType()) {
                case CHILD_ADDED:
                    System.out.println("pathchildrencache:  增加子节点------ ");
                    break;
                case CHILD_UPDATED:
                    System.out.println("pathchildrencache:  修改子节点------");
                    break;
                case CHILD_REMOVED:
                    System.out.println("pathchildrencache:  删除子节点------");
                    break;
                case CONNECTION_SUSPENDED:
                    break;
                case CONNECTION_RECONNECTED:
                    break;
                case CONNECTION_LOST:
                    break;
                case INITIALIZED:
                    break;
            }
        });

        curatorFramework.create().forPath("/xxtt", "111".getBytes());
        curatorFramework.setData().forPath("/xxtt", "6666".getBytes());
        curatorFramework.create().forPath("/xxtt/tt", "6666".getBytes());
        curatorFramework.setData().forPath("/xxtt/tt", "9999".getBytes());
        curatorFramework.delete().deletingChildrenIfNeeded().forPath("/xxtt");
        TimeUnit.SECONDS.sleep(5);
      #+END_SRC
    + TreeCache   pathcaceh+nodecache 的合体（监视路径下的创建、更新、删除事件）
  + 代码路径: https://github.com/offline7LY/demo/tree/master/zookeeper/src/main/java/com/lx/demo/curator
* zookeeper的实际应用场景
+ 分布式锁的实现
+ socket + zookeeper模拟注册中心主从调用
